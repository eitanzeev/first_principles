{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.336362190988558\n"
     ]
    }
   ],
   "source": [
    "# here is a mathematical expression that takes 3 inputs and produces one output\n",
    "from math import sin, cos\n",
    "\n",
    "def f(a, b, c):\n",
    "  return -a**3 + sin(3*b) - 1.0/c + b**2.5 - a**0.5\n",
    "\n",
    "print(f(2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK for dim 0: expected -12.353553390593273, yours returns -12.353553390593273\n",
      "OK for dim 1: expected 10.25699027111255, yours returns 10.25699027111255\n",
      "OK for dim 2: expected 0.0625, yours returns 0.0625\n"
     ]
    }
   ],
   "source": [
    "# write the function df that returns the analytical gradient of f\n",
    "# i.e. use your skills from calculus to take the derivative, then implement the formula\n",
    "# if you do not calculus then feel free to ask wolframalpha, e.g.:\n",
    "# https://www.wolframalpha.com/input?i=d%2Fda%28sin%283*a%29%29%29\n",
    "\n",
    "def gradf(a, b, c):\n",
    "\n",
    "    df_da = (-3)*a**2 - (0.5)*(a**-0.5)\n",
    "    df_db = (3)*cos(3*b) + (5/2)*(b**1.5)\n",
    "    df_dc = 1/(c**2)\n",
    "\n",
    "    return [df_da, df_db, df_dc] # todo, return [df/da, df/db, df/dc]\n",
    "\n",
    "# expected answer is the list of\n",
    "ans = [-12.353553390593273, 10.25699027111255, 0.0625]\n",
    "yours = gradf(2, 3, 4)\n",
    "for dim in range(3):\n",
    "  ok = 'OK' if abs(yours[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
    "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {yours[dim]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12.353553380251014, 10.256990368162633, 0.0624999607623522]\n",
      "OK for dim 0: expected -12.353553390593273, yours returns -12.353553380251014\n",
      "OK for dim 1: expected 10.25699027111255, yours returns 10.256990368162633\n",
      "OK for dim 2: expected 0.0625, yours returns 0.0624999607623522\n"
     ]
    }
   ],
   "source": [
    "# now estimate the gradient numerically without any calculus, using\n",
    "# the approximation we used in the video.\n",
    "# you should not call the function df from the last cell\n",
    "\n",
    "# -----------\n",
    "numerical_grad = [0, 0, 0] # TODO\n",
    "# -----------\n",
    "## a section\n",
    "a = 2\n",
    "b = 3\n",
    "c = 4\n",
    "h = 0.00000001\n",
    "\n",
    "\n",
    "fa1 = f(a, b, c)\n",
    "fa2 = f(a+h,b,c)\n",
    "\n",
    "numerical_grad[0] = (fa2-fa1)/h\n",
    "\n",
    "#b section\n",
    "fb1 = f(a, b, c)\n",
    "fb2 = f(a,b+h,c)\n",
    "\n",
    "numerical_grad[1] = (fb2-fb1)/h\n",
    "\n",
    "#c section\n",
    "fc1 = f(a, b, c)\n",
    "fc2 = f(a,b,c+h)\n",
    "\n",
    "numerical_grad[2] = (fc2-fc1)/h\n",
    "\n",
    "\n",
    "print(numerical_grad)\n",
    "for dim in range(3):\n",
    "  ok = 'OK' if abs(numerical_grad[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
    "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad[dim]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK for dim 0: expected -12.353553390593273, yours returns -12.353553291433172\n",
      "OK for dim 1: expected 10.25699027111255, yours returns 10.256990368162633\n",
      "OK for dim 2: expected 0.0625, yours returns 0.0624999607623522\n"
     ]
    }
   ],
   "source": [
    "# there is an alternative formula that provides a much better numerical\n",
    "# approximation to the derivative of a function.\n",
    "# learn about it here: https://en.wikipedia.org/wiki/Symmetric_derivative\n",
    "# implement it. confirm that for the same step size h this version gives a\n",
    "# better approximation.\n",
    "\n",
    "# -----------\n",
    "numerical_grad2 = [0, 0, 0] # TODO\n",
    "# -----------\n",
    "\n",
    "# -----------\n",
    "## a section\n",
    "a = 2\n",
    "b = 3\n",
    "c = 4\n",
    "h = 0.00000001\n",
    "\n",
    "\n",
    "fa1 = f(a-h, b, c)\n",
    "fa2 = f(a+h,b,c)\n",
    "\n",
    "numerical_grad2[0] = (fa2-fa1)/(2*h)\n",
    "\n",
    "#b section\n",
    "fb1 = f(a, b-h, c)\n",
    "fb2 = f(a,b+h,c)\n",
    "\n",
    "numerical_grad2[1] = (fb2-fb1)/(2*h)\n",
    "\n",
    "#c section\n",
    "fc1 = f(a, b, c-h)\n",
    "fc2 = f(a,b,c + h)\n",
    "\n",
    "numerical_grad2[2] = (fc2-fc1)/(2*h)\n",
    "\n",
    "for dim in range(3):\n",
    "  ok = 'OK' if abs(numerical_grad2[dim] - ans[dim]) < 1e-5 else 'WRONG!'\n",
    "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {numerical_grad2[dim]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## section 2: support for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.000000000008441\n"
     ]
    }
   ],
   "source": [
    "# Value class starter code, with many functions taken out\n",
    "from math import log\n",
    "import math\n",
    "\n",
    "class Value:\n",
    "\n",
    "    def __init__(self, data, _children=(), _op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = lambda: None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "\n",
    "    def __radd__(self, other): #fallback for value + integer vs integer + value. \n",
    "        return self + other\n",
    "\n",
    "\n",
    "    def __rmul__(self, other): #fallback for value * integer vs integer * value. If python values, it will try the other order using __rmul__ internal method\n",
    "        return self * other\n",
    "    \n",
    "    def __truediv__(self, other): #Redefining division as an application of x**k, where k = -1. x**-1 = (1/x)\n",
    "        return self * (other ** -1)\n",
    "    \n",
    "    def __neg__(self): #turns self negative\n",
    "        return self * -1\n",
    "    \n",
    "    def __add__(self, other): # exactly as in the video\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        #set the backward function here\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self,other):\n",
    "        other = other if isinstance(other, Value) else Value(other) #helps force data to be values in case we are working with Value, Integer operations\n",
    "        out = Value(self.data*other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        \n",
    "        #set the backward function here\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        n = self.data \n",
    "        t_val = (math.exp(2*n) - 1) / (math.exp(2*n) + 1)\n",
    "        out = Value(t_val, (self, ), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 - t_val**2)\n",
    "\n",
    "        #the backward function here\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def exp(self): \n",
    "        x = self.data \n",
    "        out  = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad * out.data #out.data is the derivative because d/dx(exp(x)) =  exp(x)\n",
    "           \n",
    "\n",
    "        #the backward function here\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def log(self):\n",
    "        x = self.data\n",
    "        out = Value(math.log(x), (self, ), 'log')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += ((x**-1))*out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int,float)), \"only handle integer and float values for the calculation of power values\"\n",
    "        out = Value(self.data**other, (self, ), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other*((self.data)**(other-1)))*out.grad #out.grad portion is reponsible for 'chaining' the calculation of the gradient\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        #The topological function here allows to chain together \n",
    "        #the calculation of backpward propagation/associated gradients across our connected nodes\n",
    "        #This is creating a list of nodes such that each node is added after it's children are added.\n",
    "        #Thus we calculated gradients of change backwards from children (right to left in topological format)\n",
    "        \"\"\"\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0 #initialize self.grad to 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward() \n",
    "\n",
    "\n",
    "\n",
    "# ------\n",
    "# re-implement all the other functions needed for the exercises below\n",
    "# your code here\n",
    "# TODO\n",
    "# ------\n",
    "\n",
    "\n",
    "##Manual example of gradient calculation\n",
    "h = 0.0001\n",
    "a = Value(2.0, label = 'a')\n",
    "b = Value(-3.0, label = 'b')\n",
    "c = Value(10.0, label = 'c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "\n",
    "f = Value(-2.0, label = 'f')\n",
    "\n",
    "L = d * f; L.label = 'L'\n",
    "L1 = L.data\n",
    "\n",
    "\n",
    "a = Value(2.0, label = 'a')\n",
    "b = Value(-3.0 + h, label = 'b')\n",
    "c = Value(10.0, label = 'c')\n",
    "e = a*b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label = 'f')\n",
    "\n",
    "L = d * f; L.label = 'L'\n",
    "L2 = L.data\n",
    "\n",
    "print((L2 - L1)/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input values\n",
    "x1 = Value(2.0, label = 'x1')\n",
    "x2 = Value(0.0, label = 'x2')\n",
    "\n",
    "#weights\n",
    "w1 = Value(-3.0, label = 'w1')\n",
    "w2 = Value(1.0, label = 'w2')\n",
    "\n",
    "#bias of the neuron\n",
    "b = Value(6.88137, label = \"b\")\n",
    "\n",
    "x1w1 = x1*w1; label = 'x1*w1'\n",
    "x2w2 = x2*w2; label = 'x2*w2'\n",
    "\n",
    "#full input\n",
    "full_input = x1w1 + x2w2; full_input.label = \"x1*w1 + x2*w2\"\n",
    "n = full_input + b; n.label = 'neuron'\n",
    "\n",
    "o = n.tanh(); o.label = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure we initialize o to 1.0 before running.backward()\n",
    "o.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what should n grad be?\n",
    "n.grad #default should be 0.5. Why?\n",
    "\n",
    "#what should o grad be?\n",
    "o.grad \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient experiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.grad = 1.0\n",
    "o._backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.000005072818119"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n._backward()\n",
    "n.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5000076092271784"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b._backward()\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5000076092271784"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_input._backward()\n",
    "full_input.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.000010145636238"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2w2._backward()\n",
    "x2w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.000010145636238"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1w1._backward()\n",
    "x1w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.5000380461358915"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1._backward()\n",
    "x1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.000025364090595"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1._backward()\n",
    "w1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5000126820452975"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2._backward()\n",
    "x2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2._backward()\n",
    "w2.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input values\n",
    "x1 = Value(2.0, label = 'x1')\n",
    "x2 = Value(0.0, label = 'x2')\n",
    "\n",
    "#weights\n",
    "w1 = Value(-3.0, label = 'w1')\n",
    "w2 = Value(1.0, label = 'w2')\n",
    "\n",
    "#bias of the neuron\n",
    "b = Value(6.88137, label = \"b\")\n",
    "\n",
    "x1w1 = x1*w1; label = 'x1*w1'\n",
    "x2w2 = x2*w2; label = 'x2*w2'\n",
    "\n",
    "#full input\n",
    "full_input = x1w1 + x2w2; full_input.label = \"x1*w1 + x2*w2\"\n",
    "n = full_input + b; n.label = 'neuron'\n",
    "\n",
    "o = n.tanh(); o.label = 'output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Value(data=0.04177257051535045), Value(data=0.839024507462532), Value(data=0.00565330266221633), Value(data=0.11354961935990122)]\n",
      "2.1755153626167147\n",
      "OK for dim 0: expected 0.041772570515350445, yours returns 0.041772570515350445\n",
      "OK for dim 1: expected 0.8390245074625319, yours returns 0.8390245074625319\n",
      "OK for dim 2: expected 0.005653302662216329, yours returns 0.005653302662216329\n",
      "OK for dim 3: expected -0.8864503806400986, yours returns -0.8864503806400986\n"
     ]
    }
   ],
   "source": [
    "# without referencing our code/video __too__ much, make this cell work\n",
    "# you'll have to implement (in some cases re-implemented) a number of functions\n",
    "# of the Value object, similar to what we've seen in the video.\n",
    "# instead of the squared error loss this implements the negative log likelihood\n",
    "# loss, which is very often used in classification.\n",
    "\n",
    "# this is the softmax function\n",
    "# https://en.wikipedia.org/wiki/Softmax_function\n",
    "def softmax(logits):\n",
    "  counts = [logit.exp() for logit in logits]\n",
    "  denominator = sum(counts)\n",
    "  out = [c / denominator for c in counts]\n",
    "  return out\n",
    "\n",
    "# this is the negative log likelihood loss function, pervasive in classification\n",
    "logits = [Value(0.0), Value(3.0), Value(-2.0), Value(1.0)]\n",
    "probs = softmax(logits)\n",
    "print(probs)\n",
    "loss = -probs[3].log() # dim 3 acts as the label for this input example\n",
    "loss.backward()\n",
    "print(loss.data)\n",
    "\n",
    "ans = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
    "for dim in range(4):\n",
    "  ok = 'OK' if abs(logits[dim].grad - ans[dim]) < 1e-5 else 'WRONG!'\n",
    "  print(f\"{ok} for dim {dim}: expected {ans[dim]}, yours returns {logits[dim].grad}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the gradient using the torch library\n",
    "# torch should give you the exact same gradient\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input values\n",
    "x1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\n",
    "\n",
    "#weights\n",
    "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\n",
    "\n",
    "\n",
    "#bias of the neuron\n",
    "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
    "full_input = x1*w1 + x2*w2\n",
    "n = full_input + b; n.label = 'neuron'\n",
    "o = torch.tanh(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron, Layer and MLP in micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-0.046540483011478236)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Neuron:\n",
    "\n",
    "    def __init__(self, nin):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for i in range(nin)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def __call__(self, x): #__call__ provides functionality applied at n(x)\n",
    "\n",
    "        #Create a zipped iterable for w, x for dot product wise calculation\n",
    "        neural_activation = sum(w_pos * x_pos for w_pos, x_pos in zip(self.w, x)) + self.b\n",
    "        neural_response = neural_activation.tanh() #nonlinearity function for response\n",
    "\n",
    "        return neural_response\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "    \n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nin, neurons_in_layer): \n",
    "        self.neurons = [Neuron(nin) for i in range(neurons_in_layer)]\n",
    "        #neurons in layer is the number of independent neurons we want in this layer.\n",
    "        #nin denotes the number of inputs each neuron in this layer should take.\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons] #the list of neural responses\n",
    "        \n",
    "        return outs[0] if len(outs) == 1 else outs #return single element value if outs is a single element. Else out.\n",
    "    \n",
    "    def parameters(self):\n",
    "        #return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            ps = neuron.parameters()\n",
    "            params.extend(ps)\n",
    "        return params\n",
    "\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, nin, nouts): #nouts must a list that signifies the number of neurons we want in each layer\n",
    "        size = [nin] + nouts \n",
    "        self.layers = [Layer(size[i], size[i+1]) for i in range(len(nouts))] \n",
    "\n",
    "    def __call__(self, x): # we want this function to create our layers within the MLP when we call it\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            ps = layer.parameters()\n",
    "            params.extend(ps)\n",
    "\n",
    "        return ps\n",
    "    \n",
    "# x = [2.3, 3.0] #inputs into the neuron\n",
    "# n = Neuron(2)\n",
    "# n(x)\n",
    "\n",
    "# x = [-4, 5]\n",
    "# n = Layer(2, 3)\n",
    "\n",
    "# n(x)\n",
    "\n",
    "x = [3, 5, 4, 3, 7,10]\n",
    "n = MLP(6, [4,4,1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample execution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.36007679879186466),\n",
       " Value(data=0.4857981152710343),\n",
       " Value(data=0.30140283050505345),\n",
       " Value(data=0.333976222691408)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=6.194641937141853)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to do backward propagation and calculate the gradients of each neuron in each layer. These gradients are then used to adjust the weights for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How many weights are on the 2nd neuron of the first layer\n",
    "len(n.layers[0].neurons[1].w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.1469797157632837"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#What is the gradient of the 3rd weight of the 2nd neuron of the first layer?\n",
    "#negative grads would mean that increasing this weight will make the loss go down. Influence on loss is negative (numerical)\n",
    "#positive grads would mean that increasing this weight will make the loss go up. Influence on loss is positive (numerical)\n",
    "n.layers[0].neurons[1].w[2].grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we want to adjust weights using gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in n.parameters():\n",
    "    p.data += -0.01 * p.grad #minimize the loss. REal number is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Value(data=-0.24063122334875564),\n",
       " Value(data=-0.11629472115993122),\n",
       " Value(data=0.06803411789194198),\n",
       " Value(data=0.09521259133832294)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = [n(x) for x in xs]\n",
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=4.279437984051564)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full training loop by hand with micrograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "ys = [1.0, -1.0, -1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  position through loop\n",
      "0 4.339661626111346\n",
      "1 4.200537490613689\n",
      "2 4.072553120756764\n",
      "3 3.928985589583614\n",
      "4 3.764181918058666\n",
      "5  position through loop\n",
      "5 3.5733751340984545\n",
      "6 3.353000859143111\n",
      "7 3.1029291264379375\n",
      "8 2.829193749096187\n",
      "9 2.545069739021252\n",
      "10  position through loop\n",
      "10 2.268008312506775\n",
      "11 2.013428880326604\n",
      "12 1.790185060781662\n",
      "13 1.6003574569760963\n",
      "14 1.4416042497128316\n",
      "15  position through loop\n",
      "15 1.309621221351183\n",
      "16 1.1997420647173411\n",
      "17 1.1077182709847535\n",
      "18 1.0299860204173767\n",
      "19 0.9636785424312667\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    if k % 5 == 0:\n",
    "        print(k, ' position through loop')\n",
    "\n",
    "    #Forward pass\n",
    "    ypred = [n(x) for x in xs]\n",
    "    #loss calculation within the forward pass\n",
    "    loss = sum((yout - ygt)**2 for yout, ygt in zip(ypred, ys))\n",
    "    \n",
    "    # #loss break\n",
    "    # if loss.data < .5:\n",
    "    #     break\n",
    "\n",
    "    \"\"\"#MAKE SURE TO FLUSH the grad.\n",
    "        If we don't flush the grads then the model doesn't learn anything.\n",
    "        It's just adding grads from different loops\n",
    "    \"\"\"\n",
    "\n",
    "    for p in n.parameters():\n",
    "       p.grad = 0.0\n",
    "\n",
    "    #backward pass to calculate grads from forward pass. Grads tell us how changing the weight mattered!\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    for p in n.parameters():#MAKE SURE TO FLUSH the grad\n",
    "        p.data += -0.05 * p.grad\n",
    "\n",
    "    print(k, loss.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Torch for softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the gradient using the torch library\n",
    "# torch should give you the exact same gradient\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1755, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "Latest Loss:  2.1755153626167143\n",
      "Grads:  tensor([ 0.0418,  0.8390,  0.0057, -0.8865], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logits = torch.Tensor([0.0, 3.0, -2.0, 1.0]).double(); logits.requires_grad = True\n",
    "gradient = [0.041772570515350445, 0.8390245074625319, 0.005653302662216329, -0.8864503806400986]\n",
    "\n",
    "# softmax_probs = F.softmax(logits)\n",
    "# print(probs)\n",
    "# loss = -softmax_probs[3].log()\n",
    "# print(loss, '---- loss')\n",
    "\n",
    "loss = -F.log_softmax(logits, dim = -1)[3]\n",
    "print(loss)\n",
    "\n",
    "#now update the gradients\n",
    "loss.backward()\n",
    "\n",
    "print(\"Latest Loss: \", loss.data.item())\n",
    "\n",
    "print(\"Grads: \", logits.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-alpha-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
