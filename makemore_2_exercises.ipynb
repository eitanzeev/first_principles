{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - improving on 2.2 test loss from Andrej Karpathy's video\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training, dev, and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_production(data, block_size = 3):\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for word in data:\n",
    "\n",
    "        context = [0] * block_size\n",
    "\n",
    "        for char in word + '.':\n",
    "\n",
    "\n",
    "            X.append(context)\n",
    "            Y.append(stoi[char])\n",
    "\n",
    "            context = context[1:] + [stoi[char]]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_limit = int(0.80 * len(words))\n",
    "test_limit = int(0.90 * len(words))\n",
    "\n",
    "train_data= words[:train_limit]\n",
    "dev_data = words[train_limit:test_limit]\n",
    "test_data = words[test_limit:]\n",
    "\n",
    "Xtr, Ytr = tensor_production(train_data, block_size = 5)\n",
    "Xdev, Ydev = tensor_production(dev_data, block_size = 5)\n",
    "Xtest, Ytest = tensor_production(test_data, block_size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  5],\n",
       "        [ 0,  0,  0,  5, 13],\n",
       "        ...,\n",
       "        [ 0,  0,  1, 13,  9],\n",
       "        [ 0,  1, 13,  9, 18],\n",
       "        [ 1, 13,  9, 18, 18]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22633, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xdev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22735, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now set up the dimensions of the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_parameters(embedding_dim = 10, block_size = 5, layer1_dim = 100):\n",
    "\n",
    "    g = torch.Generator().manual_seed(2147483647)\n",
    "    C = torch.randn((len(stoi.keys()), embedding_dim))\n",
    "    W1 = torch.randn((block_size*embedding_dim, layer1_dim))\n",
    "    b1 = torch.randn(layer1_dim)\n",
    "    W2 = torch.randn((layer1_dim, 27))\n",
    "    b2 = torch.randn(27)\n",
    "\n",
    "   parameters = [C,W1, b1, W2, b2]\n",
    "\n",
    "    for p in parameters:\n",
    "        p.requires_grad = True\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets define the training loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(X_submit,\n",
    "                Y_submit,\n",
    "                batch_size,\n",
    "                iterations,\n",
    "                parameters,\n",
    "                discover_learning_rate = False,\n",
    "                show_loss_steps = False,\n",
    "                learning_rate_submit = 0.1):\n",
    "\n",
    "    if discover_learning_rate and not learning_rate_submit:\n",
    "        learning_rate_range = torch.linspace(-3, 0, iterations)\n",
    "\n",
    "        lri = 10**learning_rate_range\n",
    "    else:\n",
    "        assert learning_rate_submit is not None, \"Running model without learning check again!\"\n",
    "\n",
    "\n",
    "\n",
    "    store_lr_perf = []\n",
    "    store_loss = []\n",
    "    steps = []\n",
    "\n",
    "    for it in range(iterations):\n",
    "\n",
    "        batch = torch.randint(0, X_submit.shape[0], (batch_size,))\n",
    "\n",
    "        embedding = C[X_submit[batch]]\n",
    "\n",
    "        tanh = torch.tanh(embedding.view(-1,50) @ W1 + b1)\n",
    "\n",
    "        logits = tanh@W2 + b2 \n",
    "\n",
    "        loss = F.cross_entropy(logits, Y_submit[batch])\n",
    "\n",
    "\n",
    "        #We need to reset the gradients before we run backward so that we don't compile grads\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "\n",
    "        store_loss.append(loss.item())\n",
    "        steps.append(it)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        if discover_learning_rate and not learning_rate_submit:\n",
    "            learning_rate = lri[it]\n",
    "            store_lr_perf.append(learning_rate)\n",
    "        else:\n",
    "            learning_rate = learning_rate_submit\n",
    "\n",
    "        #Now update the parameters\n",
    "        for p in parameters:\n",
    "            p.data += -learning_rate * p.grad\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"Training Loss: {loss.item()}\")\n",
    "\n",
    "    if discover_learning_rate and not learning_rate_submit:\n",
    "        print(f\"Learning Rate storage shape{len(store_lr_perf)}\")\n",
    "        print(f\"Loss storage shape{len(store_loss)}\")  \n",
    "        plt.title(\"Loss per learning rate\")\n",
    "        plt.plot(store_lr_perf, store_loss)\n",
    "\n",
    "        if show_loss_steps:\n",
    "            plt.clf()\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    if show_loss_steps:\n",
    "        plt.title(\"Loss per step\")\n",
    "        plt.plot(steps, store_loss)\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = training_parameters(embedding_dim = 20,\n",
    "                    block_size = 5,\n",
    "                    layer1_dim = 100)\n",
    "\n",
    "\n",
    "C  =  parameters[0]\n",
    "W1 = parameters[1]\n",
    "b1 = parameters[2]\n",
    "W2 = parameters[3]\n",
    "b2 = parameters[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 2.6361582279205322\n"
     ]
    }
   ],
   "source": [
    "training_loop(X_submit = Xtr,\n",
    "            Y_submit = Ytr,\n",
    "            batch_size = 100,\n",
    "            iterations = 10000,\n",
    "            discover_learning_rate=False,\n",
    "            show_loss_steps=False,\n",
    "            learning_rate_submit = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see learning rate is working best when aroudn ~0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now lets run on dev set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in dev 3.0618863105773926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embedding = C[Xdev]\n",
    "tanh = torch.tanh(embedding.view(-1,50) @ W1 + b1)\n",
    "\n",
    "logits = tanh@W2 + b2 \n",
    "\n",
    "loss_dev = F.cross_entropy(logits, Ydev)\n",
    "\n",
    "print(f\"Loss in dev {loss_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in dev 3.052267551422119\n"
     ]
    }
   ],
   "source": [
    "embedding = C[Xtest]\n",
    "tanh = torch.tanh(embedding.view(-1,50) @ W1 + b1)\n",
    "\n",
    "logits = tanh@W2 + b2 \n",
    "\n",
    "loss_test = F.cross_entropy(logits, Ytest)\n",
    "\n",
    "print(f\"Loss in dev {loss_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a-alpha-conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
